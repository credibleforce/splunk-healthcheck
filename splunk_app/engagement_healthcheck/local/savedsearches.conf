[license_usage_by_sourcetype]
search = index=_internal  source=*license_usage.log* type="Usage" \
| eval h=if(len(h)=0 OR isnull(h),"(SQUASHED)",h) \
| eval s=if(len(s)=0 OR isnull(s),"(SQUASHED)",s) \
| eval idx=if(len(idx)=0 OR isnull(idx),"(UNKNOWN)",idx) \
| bin _time span=1h \
| stats sum(b) as b by _time, pool, s, st, h, idx \
| eval GB=round(b/1024/1024/1024,2) \
| timechart span=1h sum(GB) AS volumeGB by st fixedrange=false

[search_volume]
search = (search_id!="rsa_*" action=search index=_audit sourcetype=audittrail (host=*))  \
| eval user=if((user == "n/a"),null(),user), search_type=case(match(search_id,"^SummaryDirector_"),"summarization",match(savedsearch_name,"^_ACCELERATE_"),"acceleration",match(search_id,"^((rt_)?scheduler_|alertsmanager_)"),"scheduled",match(search_id,"\\d{10}\\.\\d+(_[0-9A-F]{8}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{12})?$$"),"ad hoc",true(),"other"), search=if((isnull(savedsearch_name) OR (savedsearch_name == "")),search,savedsearch_name)  \
| where isnotnull(search)  \
| timechart span=1h count as "search count" by host

[cpu_usage_overview]
search = index=_introspection sourcetype=splunk_resource_usage component=PerProcess host=* data.pct_cpu=*  \
| eval pid = 'data.pid' \
| eval pct_cpu = 'data.pct_cpu' \
| bin _time span=1h  \
| stats latest(pct_cpu) AS resource_usage_dedup by host, pid, _time  \
| stats sum(resource_usage_dedup) AS resource_usage by host, _time  \
| timechart minspan=1h First(resource_usage) AS "Resource Usage" by host

[memory_usage_overview]
search = (component=PerProcess host=* index=_introspection sourcetype=splunk_resource_usage) data.mem_used=* \
| bin _time span=1h  \
| eval pid='data.pid', mem_used='data.mem_used'  \
| stats latest(mem_used) AS resource_usage_dedup latest(process_class) AS process_class by pid, _time, host  \
| stats sum(resource_usage_dedup) AS resource_usage by _time, host \
| timechart minspan=1h First(resource_usage) AS "Resource Usage" by host

[splunk_servers]
search = | rest splunk_server_group="*" /services/server/info \
   | fields + splunk_server, product_type, version, server_roles, os_name_extended, os_version, cpu_arch, numberOfCores, numberOfVirtualCores, physicalMemoryMB, fips_mode, isForwarding, isFree, isTrial, kvStoreStatus, licenseState, master_uri,

[active_users]
search = (user!=cmon_user user!="splunk-system-user" action=search index=_audit info=completed (host=*))  \
| stats dc(user) as "Distinct Users", values(user) AS users by host

[splunk_server_settings]
search = | rest /servicesNS/-/-/server/settings splunk_server=*  \
|  table splunk_server, SPLUNK_DB, SPLUNK_HOME, appServerPorts, enableSplunkWebSSL, httpport, kvStoreDisabled, kvStorePort, mgmtHostPort, minFreeSpace, pass4SymmKey, serverName, sessionTimeout, startwebserver, trustedIP

[heavy_forwarder_volume]
search = | inputlookup dmc_forwarder_assets  \
| makemv delim=" " avg_tcp_kbps_sparkline  \
| eval sum_kb = if (status == "missing", "N/A", sum_kb)  \
| eval avg_tcp_kbps_sparkline = if (status == "missing", "N/A", avg_tcp_kbps_sparkline)  \
| eval avg_tcp_kbps = if (status == "missing", "N/A", avg_tcp_kbps)  \
| eval avg_tcp_eps = if (status == "missing", "N/A", avg_tcp_eps)  \
| eval forwarder_type = case(forwarder_type == "full", "Heavy Forwarder", forwarder_type == "uf", "Universal Forwarder", forwarder_type == "lwf", "Light Forwarder", 1==1, forwarder_type)  \
| eval last_connected = strftime(last_connected, "%m/%d/%Y %H:%M:%S %z")  \
| search NOT  \
    [| rest splunk_server=* /servicesNS/-/-/server/info search="server_roles=search_head OR server_roles=indexer"  \
    | table host_fqdn  \
    | rename host_fqdn AS hostname]  \
| fields hostname, forwarder_type, version, os, arch, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps  \
| search hostname="***"  \
| search status="*"  \
| rename hostname as Instance, forwarder_type as Type, version as Version, os as OS, arch as Architecture, status as Status, last_connected as "Last Connected to Indexers", sum_kb as "Total KB", avg_tcp_kbps_sparkline as "Average KB/s Over Time", avg_tcp_kbps as "Average KB/s", avg_tcp_eps as "Average Events/s"  \
| search Type="Heavy Forwarder"  \
| stats max("Average KB/s") AS "Average KB/s", max("Average Events/s") AS "Average Events/s" by Instance  \
| sort - "Average KB/s"

[scheduler_summary]
search = | rest splunk_server_group=dmc_group_search_head splunk_server_group="*" /servicesNS/-/-/saved/searches search="is_scheduled=1" search="disabled=0"  \
| stats count by splunk_server  \
| join type=outer splunk_server  \
    [| rest splunk_server_group=dmc_group_search_head splunk_server_group="*" /services/server/status/limits/search-concurrency  \
    | fields + splunk_server, max_hist_scheduled_searches, max_rt_scheduled_searches]  \
| join type=outer splunk_server  \
    [| rest splunk_server_group=dmc_group_search_head splunk_server_group="*" /services/server/status/resource-usage/splunk-processes  \
    | search ("search_props.role"="head" ("data.search_props.type"="datamodel acceleration" OR "search_props.type"="datamodel acceleration" OR "data.search_props.type"="report acceleration" OR "search_props.type"="report acceleration" OR "data.search_props.type"="scheduled" OR "search_props.type"="scheduled" OR "data.search_props.type"="summary indexing" OR "search_props.type"="summary indexing"))  \
    | dedup search_props.sid  \
    | stats count(eval('search_props.mode'=="historical batch" OR 'search_props.mode'=="historical")) as count_hist_search, count(eval('search_props.mode'=="RT" OR 'search_props.mode'=="RT indexed")) as count_rt_search by splunk_server]  \
| join type=outer splunk_server  \
    [| rest splunk_server_group=dmc_group_search_head splunk_server_group="*" /services/server/info  \
    | fields + splunk_server, numberOfCores, numberOfVirtualCores]  \
| eval count_hist_search=if(isnull(count_hist_search),0,count_hist_search)  \
| eval count_rt_search=if(isnull(count_rt_search),0,count_rt_search)  \
| eval hist_concur_vs_limit=((count_hist_search . " / ") . max_hist_scheduled_searches)  \
| eval rt_concur_vs_limit=((count_rt_search . " / ") . max_rt_scheduled_searches)  \
| eval core_info=((if(isnull(numberOfCores),"N/A",numberOfCores) . " / ") . if(isnull(numberOfVirtualCores),"N/A",numberOfVirtualCores))  \
| fields + splunk_server, core_info, hist_concur_vs_limit, rt_concur_vs_limit, count  \
| rename core_info as "CPU Cores (Physical / Virtual)", count as "Unique Scheduled Reports", hist_concur_vs_limit as "Concurrency of Historical Scheduled Report (Running/Limit)", rt_concur_vs_limit as "Concurrency of Real-time Scheduled Report (Running/Limit)", splunk_server as Instance

[data_onboarding_issues]
search = (NOT data_sourcetype=http_event_collector_metrics NOT data_sourcetype=kvstore NOT data_sourcetype=mongod NOT data_sourcetype=splunk_* NOT data_sourcetype=splunkd NOT data_sourcetype=splunkd_* index=_internal source=*splunkd.log* (host=*) (log_level=ERROR OR log_level=WARN) (component=AggregatorMiningProcessor OR component=DateParserVerbose OR component=LineBreakingProcessor))  \
| eval problem=case((component == "LineBreakingProcessor"),"Line Breaking Issues",(component == "DateParserVerbose"),"Timestamp Parsing Issues",(component == "AggregatorMiningProcessor"),"Aggregation Issues")  \
| stats count, values(host) AS splunk_server by problem  \
| sort - count

[index_cluster_bucket_status]
search = | rest /servicesNS/-/-/cluster/master/indexes splunk_server_group=dmc_group_cluster_master \
    |  foreach searchable_copies_tracker.*.actual_copies_per_slot  \
    [ eval expectedBuckets_site<<MATCHSTR>>=if('searchable_copies_tracker.<<MATCHSTR>>.expected_total_per_slot'=='searchable_copies_tracker.<<MATCHSTR>>.actual_copies_per_slot',"true","false") ] \
|   rename searchable_copies_tracker.* AS site* \
|  fields splunk_server, title,is_searchable, expectedBuckets_site*, num_buckets, site* \
| eval failureCount=0 \
| foreach expectedBuckets_site* \
    [ eval failureCount=if('expectedBuckets_site<<MATCHSTR>>'=="false",failureCount+1,failureCount) ]

[searches_by_duration]
search = index=_internal sourcetype=scheduler status=* host=*  \
| table _time app, alert_action, host, splunk_server, component, user search_type, savedsearch_name status scheduled_time run_time result_count, event_message  \
| eval splunk_server=host  \
| eval savedsearch_name=if(search_type="datamodel_acceleration",replace(savedsearch_name,"_ACCELERATE_DM|_ACCELERATE_|_"+app+"_",""),savedsearch_name)  \
| join splunk_server, app, search_type, savedsearch_name type=outer  \
    [| rest /servicesNS/-/-/saved/searches splunk_server_group=dmc_group_search_head search="is_scheduled=1 AND disabled=0"  \
    | table splunk_server, "eai*app", "eai*owner", title, triggered_alert_count, search_type, description, cron_schedule, action.email, action.summary_index.report, actions, alert.expires, author, allow_skew, disabled, dispatch.buckets, dispatch.earliest_time,dispatch.latest_time, dispatch.lookups, dispatch.max_time,is_scheduled, is_visible,max_concurrent,next_scheduled_time, qualifiedSearch,search, realtime_schedule, schedule_priority, schedule_window, updated,  \
    | rename eai:acl.app AS app, title AS savedsearch_name  \
    | eval search_type="scheduled"  \
    | table splunk_server, app, search_type, savedsearch_name, cron_schedule, schedule_window, search, qualifiedSearch]  \
| join splunk_server, app, search_type, savedsearch_name type=outer  \
    [| rest /servicesNS/-/-/data/models splunk_server_group=dmc_group_search_head search="disabled=0"  \
    | table splunk_server,title acceleration.cron_schedule eai:digest, eai:acl.app, acceleration.allow_skew, acceleration.max_time, acceleration.max_concurrent  \
    | rename title as savedsearch_name  \
    | rename acceleration.* AS *  \
    | rename eai:acl.app AS app  \
    | eval search_type="datamodel_acceleration"  \
    | table splunk_server, app, search_type, savedsearch_name, cron_schedule, allow_skew, max_time, max_concurrent]  \
| stats count, values(user) AS user, values(cron_schedule) AS cron_schedule, values(schedule_window) AS schedule_window, values(allow_skew) AS allow_skew, values(status) AS status, values(app) AS app, max(run_time) AS max_run_time, avg(run_time) AS average_run_time,sum(run_time) AS total_run_time by splunk_server,search_type, savedsearch_name  \
| sort - average_run_time

[execution_latency_overview]
search = host=* index=_internal sourcetype=scheduler status="success" | eval window_time=if(isnotnull(window_time),window_time,0), execution_latency=max((dispatch_time - (scheduled_time + window_time)),0)  \
| eval app=host+":"+app \
| bucket _time span=1d \
| stats avg(execution_latency) AS average_latency, max(execution_latency) AS max_latency by host,_time \
| eval average_latency=round(average_latency,0)

[scheduled_search_concurrency_as_percentage_of_max]
search = | rest /servicesNS/-/-/saved/searches splunk_server_group=dmc_group_search_head search="is_scheduled=1 AND disabled=0"  \
| table splunk_server, eai:acl.app, title, cron_schedule, schedule_window, next_scheduled_time  \
| join type=left splunk_server  \
    [| rest splunk_server_group=dmc_group_search_head /services/server/status/limits/search-concurrency  \
    | fields splunk_server, max_hist_scheduled_searches, max_rt_scheduled_searches]  \
| eval scheduled_search="["+ cron_schedule +"] " + 'eai:acl.app'+":"+title  \
| eval overlap_type=case(match(cron_schedule,"^\* \* \* \* \*"),"0,5,10,15,20,25,30,35,40,45,50,55",match(cron_schedule,"^\*/5 \*"),"0,5,10,15,20,25,30,35,40,45,50,55",match(cron_schedule,"^\*/10 \*"),"0,10,20,30,40,50",match(cron_schedule,"^\*/15 \*"),"0,15,30,45",match(cron_schedule,"^0 [0-9\*]{1,2}"),"0")  \
| eval overlap_minute=split(overlap_type,",")  \
| stats dc(scheduled_search) AS scheduled_search_count, values(scheduled_search) AS scheduled_search, max(max_hist_scheduled_searches) AS max_hist_scheduled_searches by splunk_server, schedule_window, overlap_minute  \
| eval percentage_of_max=round((scheduled_search_count/max_hist_scheduled_searches)*100,2)  \
|  sort - percentage_of_max  \
| chart max(percentage_of_max) AS percentage_of_max over overlap_minute by splunk_server

[scheduled_searches_high_concurrency_by_minute]
search = | rest /servicesNS/-/-/saved/searches splunk_server_group=dmc_group_search_head search="is_scheduled=1 AND disabled=0"  \
| table splunk_server, eai:acl.app, title, cron_schedule, schedule_window, next_scheduled_time  \
| join type=left splunk_server  \
    [| rest splunk_server_group=dmc_group_search_head /services/server/status/limits/search-concurrency  \
    | fields splunk_server, max_hist_scheduled_searches, max_rt_scheduled_searches]  \
| eval scheduled_search="["+ cron_schedule +"] " + 'eai:acl.app'+":"+title  \
| eval overlap_type=case(match(cron_schedule,"^\* \* \* \* \*"),"0,5,10,15,20,25,30,35,40,45,50,55",match(cron_schedule,"^\*/5 \*"),"0,5,10,15,20,25,30,35,40,45,50,55",match(cron_schedule,"^\*/10 \*"),"0,10,20,30,40,50",match(cron_schedule,"^\*/15 \*"),"0,15,30,45",match(cron_schedule,"^0 [0-9\*]{1,2}"),"0")  \
| eval overlap_minute=split(overlap_type,",")  \
| stats dc(scheduled_search) AS scheduled_search_count, values(scheduled_search) AS scheduled_search, max(max_hist_scheduled_searches) AS max_hist_scheduled_searches by splunk_server, schedule_window, overlap_minute  \
| eval percentage_of_max=round((scheduled_search_count/max_hist_scheduled_searches)*100,2)  \
|  sort - percentage_of_max  \
| where percentage_of_max>75

[searches_exceeding_frequency]
search = index=_internal sourcetype=scheduler status=* host=* run_time>60 \
| table _time app, alert_action, host, splunk_server, component, user search_type, savedsearch_name status scheduled_time run_time result_count, event_message  \
| eval splunk_server=host  \
| join max=0 splunk_server, app, savedsearch_name type=outer  \
    [| rest /servicesNS/-/-/saved/searches splunk_server_group=dmc_group_search_head search="is_scheduled=1 AND disabled=0" \
    | table splunk_server, "eai*app", "eai*owner", title, triggered_alert_count, description, cron_schedule, action.email, action.summary_index.report, actions, alert.expires, author, allow_skew, disabled, dispatch.buckets, dispatch.earliest_time,dispatch.latest_time, dispatch.lookups, dispatch.max_time,is_scheduled, is_visible,max_concurrent,next_scheduled_time, qualifiedSearch,search, realtime_schedule, schedule_priority, schedule_window, updated,  \
    | rename eai:acl.app AS app, title AS savedsearch_name  \
    | table app, splunk_server, savedsearch_name, cron_schedule, search]  \
| search (run_time>600 AND cron_schedule="*/5 *") OR (run_time<600 AND run_time>60 AND NOT cron_schedule="*/5 *") \
| regex cron_schedule="^(\*|\*/5) \* \* \* \*"  \
| dedup savedsearch_name

[datamodel_incomplete_datamodels]
search = | rest /servicesNS/-/-/admin/summarization by_tstats=t splunk_server=* count=0  \
| eval datamodel=replace('summary.id',"DM_".'eai:acl.app'."_","")  \
| join type=left datamodel splunk_server  \
    [| rest /servicesNS/-/-/data/models splunk_server=* count=0  \
    | table splunk_server,title acceleration.cron_schedule eai:digest  \
    | rename title as datamodel  \
    | rename acceleration.cron_schedule AS cron ]  \
| table splunk_server, datamodel eai:acl.app summary.access_count, summary.access_time summary.is_inprogress summary.size summary.latest_time summary.complete summary.buckets_size summary.buckets cron summary.last_error summary.time_range summary.id summary.mod_time eai:digest summary.earliest_time summary.last_sid summary.access_count, search  \
| rename summary.id AS summary_id, summary.time_range AS retention, summary.earliest_time as earliest, summary.latest_time as latest, eai:digest as digest  \
| rename summary.* AS *, eai:acl.* AS *  \
| sort datamodel  \
| rename access_count AS Datamodel_Acceleration.access_count access_time AS Datamodel_Acceleration.access_time app AS Datamodel_Acceleration.app buckets AS Datamodel_Acceleration.buckets buckets_size AS Datamodel_Acceleration.buckets_size cron AS Datamodel_Acceleration.cron complete AS Datamodel_Acceleration.complete datamodel AS Datamodel_Acceleration.datamodel digest AS Datamodel_Acceleration.digest earliest AS Datamodel_Acceleration.earliest is_inprogress AS Datamodel_Acceleration.is_inprogress last_error AS Datamodel_Acceleration.last_error last_sid AS Datamodel_Acceleration.last_sid latest AS Datamodel_Acceleration.latest mod_time AS Datamodel_Acceleration.mod_time retention AS Datamodel_Acceleration.retention size AS Datamodel_Acceleration.size summary_id AS Datamodel_Acceleration.summary_id  \
| rename "Datamodel_Acceleration.*" as *  \
| join type=outer splunk_server, last_sid  \
    [| rest splunk_server=* count=0 /servicesNS/-/-/search/jobs reportSearch=summarize*  \
    | rename sid as last_sid  \
    | fields splunk_server, last_sid,runDuration ]  \
| eval size(MB)=round(size/1048576,1)  \
| eval retention(days)=if(retention==0,"unlimited",retention/86400)  \
| eval complete(%)=round(complete*100,1)  \
| eval runDuration(s)=round(runDuration,1)  \
| sort 100 + datamodel  \
| fields splunk_server, datamodel,app,search, cron,retention(days),earliest,latest,is_inprogress,complete(%),size(MB),runDuration(s),access_time, access_count, last_error  \
| where 'complete(%)'<100  \
| eval datamodel=datamodel+" ("+'complete(%)'+"%)" \
| stats values(datamodel) AS datamodels, dc(datamodel) AS datamodel_count by splunk_server, app

[indexes_summary]
search = | rest /servicesNS/-/-/data/indexes splunk_server_group=dmc_group_indexer  \
| search title!=_*  \
| table author, coldPath, coldPath.maxDataSizeMB, coldPath_expanded, currentDBSizeMB, datatype, defaultDatabase, disabled, eai:acl.app, eai:acl.owner, frozenTimePeriodInSecs, homePath, homePath.maxDataSizeMB, homePath_expanded, maxHostSpanSecs, totalEventCount, minTime, maxTime, maxDataSize, maxTotalDataSizeMB, maxWarmDBCount, splunk_server, summaryHomePath, summaryHomePath_expanded, thawedPath, thawedPath_expanded, title, tstatsHomePath, tstatsHomePath_expanded, updated  \
| where disabled=0  \
| eval mint=strptime(minTime,"%Y-%m-%dT%H:%M"),maxt=strptime(maxTime,"%Y-%m-%dT%H:%M"), retentionDays=round((maxt-mint)/86400,0), daysSinceEvent=round((now()-maxt)/86400)  \
| stats values("eai:acl.app") AS app, values(homePath) AS homePath, values(homePath_expanded) AS homePath_expanded, values(homePath.maxDataSizeMB) AS homePath.maxDataSizeMB, values(coldPath) AS coldPath, values(coldPath_expanded) AS coldPath_expanded, values(coldPath.maxDataSizeMB) AS coldPath.maxDataSizeMB, values(disabled) AS disabled, sum(totalEventCount) AS totalEventCount, values(maxDataSize) AS maxDataSize, values(maxTotalDataSizeMB) AS maxTotalDataSizeMB, sum(currentDBSizeMB) AS currentDBSizeMB, values(frozenTimePeriodInSecs) AS frozenTimePeriodInSecs, max(minTime) AS minTime, max(maxTime) AS maxTime, max(retentionDays) AS rententionDays, max(daysSinceEvent) AS daysSinceEvent by splunk_server, title

[index_volume_summary]
search = | rest /servicesNS/-/-/data/index-volumes splunk_server_group=dmc_group_indexer \
| search title!=_*  \
| table splunk_server, title, max_size, total_size, volume_path

[indexes_with_time_skew]
search = index=* earliest=-30m latest=now  \
| fields _time, splunk_server, host, sourcetype, _indextime, index, splunk_server \
| eval indextime=strftime(_indextime,"%Y-%m-%dT%H:%M:%S.%Q"), diff=_time-_indextime \
| stats max(diff) AS diff, latest(host), latest(sourcetype) AS sourcetype by splunk_server,index  \
| sort - diff  \
|  where diff>30 OR diff<-30

[indexes_by_role_and_user]
search = | rest /servicesNS/-/-/data/indexes splunk_server=*  \
|  table splunk_server, title  \
|  rename title AS index_name  \
|  eval joinfield=if(substr(index_name,1,1)="_","I","NI")  \
|  join type=left max=0 splunk_server joinfield  \
    [| rest /servicesNS/-/-/authorization/roles splunk_server=*  \
    |  table splunk_server,title srchIndexesAllowed  \
    |  rename title AS Role \
    |  mvexpand srchIndexesAllowed \
    |  dedup Role, srchIndexesAllowed  \
    |  eval joinfield=if(substr(srchIndexesAllowed,1,1)="_","I","NI")  \
    |  rex field=srchIndexesAllowed  mode=sed "s/[*]/%/g"]  \
|  where like(index_name,srchIndexesAllowed)  \
|  table splunk_server, index_name, Role \
|  join type=left max=0 Role splunk_server  \
    [|  rest  /servicesNS/-/-/authentication/users  splunk_server=*  \
    |  table splunk_server, title , roles  \
    | mvexpand roles  \
    | rename title AS User, roles AS Role]  \
|  stats values(User) by splunk_server, index_name, Role

[role_heirarchy]
search = | rest /servicesNS/-/-/authorization/roles splunk_server=*  \
| table splunk_server, title, defaultApp, capabilities, imported_capabilities, imported_roles, srchDiskQuota, srchFilter, srchIndexesAllowed, srchIndexesDefault, srchJobsQuota, srchTimeWin  \
| table splunk_server, title, imported_roles  \
| mvexpand imported_roles  \
| stats values(title) AS role by splunk_server,imported_roles  \
| eval role=mvjoin(role,",") \
| join type=outer max=0 splunk_server, imported_roles  \
    [| rest /servicesNS/-/-/authorization/roles splunk_server=*  \
    | rename title AS imported_roles  \
    | eval capabilities=mvjoin(capabilities,","), srchIndexesAllowed=mvjoin(srchIndexesAllowed,","), srchIndexesDefault=mvjoin(srchIndexesDefault,",")  \
    | table splunk_server, imported_roles, capabilities,srchIndexesAllowed, srchIndexesDefault]  \
| rename imported_roles AS inherited_role, role AS roles , srchIndexesAllowed AS searchableIndexes, srchIndexesDefault AS defaultIndexes  \
| table splunk_server, inherited_role, roles, capabilities, defaultIndexes, searchableIndexes

[role_capability_summary]
search = | rest /servicesNS/-/-/authorization/roles splunk_server=*  \
| table splunk_server, title, defaultApp, capabilities, imported_capabilities, imported_roles, srchDiskQuota, srchFilter, srchIndexesAllowed, srchIndexesDefault, srchJobsQuota, srchTimeWin

[apps]
search = | rest /servicesNS/-/-/apps/local splunk_server_group=dmc_group_* search="disabled=0"  \
| table splunk_server, title, label, eai:acl.sharing, version, visible, update.appurl, update.checksum, update.checksum.type, update.homepage

[scheduled_search_skipped_searches]
search = index=_internal sourcetype=scheduler status=* host=*  \
| table _time app, alert_action, host, splunk_server, component, user search_type, savedsearch_name status scheduled_time run_time result_count, event_message  \
| eval splunk_server=host \
| join splunk_server, app, savedsearch_name type=inner  \
    [| rest /servicesNS/-/-/saved/searches splunk_server_group=dmc_group_search_head \
    | where is_scheduled=1 AND disabled=0  \
    | table splunk_server, "eai*app", "eai*owner", title, triggered_alert_count, description, cron_schedule, action.email, action.summary_index.report, actions, alert.expires, author, allow_skew, disabled, dispatch.buckets, dispatch.earliest_time,dispatch.latest_time, dispatch.lookups, dispatch.max_time,is_scheduled, is_visible,max_concurrent,next_scheduled_time, qualifiedSearch,search, realtime_schedule, schedule_priority, schedule_window, updated,  \
    | rename eai:acl.app AS app, title AS savedsearch_name  \
    | table splunk_server,app, savedsearch_name, cron_schedule, schedule_window, search, qualifiedSearch]  \
| stats count, values(user) AS user, values(cron_schedule) AS cron_schedule, values(schedule_window) AS schedule_window, values(status) AS status, values(app) AS app, max(run_time) AS max_run_time, avg(run_time) AS average_run_time,sum(run_time) AS total_run_time by splunk_server, savedsearch_name  \
|  where status="skipped"

[search_best_practices_index_and_sourcetype]
search = | rest /servicesNS/-/-/saved/searches splunk_server_group=dmc_group_search_head \
    | where is_scheduled=1 AND disabled=0  \
    | table splunk_server, "eai*app", "eai*owner", title, triggered_alert_count, description, cron_schedule, action.email, action.summary_index.report, actions, alert.expires, author, allow_skew, disabled, dispatch.buckets, dispatch.earliest_time,dispatch.latest_time, dispatch.lookups, dispatch.max_time,is_scheduled, is_visible,max_concurrent,next_scheduled_time, qualifiedSearch,search, realtime_schedule, schedule_priority, schedule_window, updated,  \
    | rename eai:acl.app AS app, title AS savedsearch_name  \
    | table splunk_server,app, savedsearch_name, cron_schedule, schedule_window, search, qualifiedSearch  \
|  search app!=splunk_*  \
| regex qualifiedSearch="^search " \
| rex field=qualifiedSearch "(?<search_index>index=[^ |]+)"  \
| rex field=qualifiedSearch "(?<search_sourcetype>sourcetype=[^ |]+)" \
 \
|  table search_index search_sourcetype,splunk_server, savedsearch_name, app, qualifiedSearch  \
| eval search_check=case(!isnull(search_index) AND !isnull(search_sourcetype),"index and sourcetype",!isnull(search_index),"index only",!isnull(search_sourcetype),"sourcetype only",1==1,"neither")  \
|  table splunk_server, app, savedsearch_name, search_check, qualifiedSearch  \
| eval app=splunk_server+":"+app \
|  chart limit=100 count(search_check) over app by search_check

[data_inputs_index_mapping]
search = |  rest  /servicesNS/-/-/data/inputs/all/  \
|  table splunk_server, eai:acl.app, eai:type,eai:location, disabled, id, index, sourcetype  \
| where disabled=0 \
| stats dc(id) AS input_count, values(id) AS input by splunk_server, index, disabled  \
|  sort - input_count

[forwarder_status]
search = | inputlookup dmc_forwarder_assets  \
| makemv delim=" " avg_tcp_kbps_sparkline  \
| eval sum_kb = if (status == "missing", "N/A", sum_kb)  \
| eval avg_tcp_kbps_sparkline = if (status == "missing", "N/A", avg_tcp_kbps_sparkline)  \
| eval avg_tcp_kbps = if (status == "missing", "N/A", avg_tcp_kbps)  \
| eval avg_tcp_eps = if (status == "missing", "N/A", avg_tcp_eps)  \
| eval forwarder_type = case(forwarder_type == "full", "Heavy Forwarder", forwarder_type == "uf", "Universal Forwarder", forwarder_type == "lwf", "Light Forwarder", 1==1, forwarder_type)  \
| eval last_connected = strftime(last_connected, "%m/%d/%Y %H:%M:%S %z")  \
| search NOT  \
    [| rest splunk_server=* /servicesNS/-/-/server/info search="server_roles=search_head OR server_roles=indexer"  \
    | table host_fqdn  \
    | rename host_fqdn AS hostname]  \
| fields hostname, forwarder_type, version, os, arch, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps  \
| search hostname="***"  \
| search status="*"  \
| rename hostname as Instance, forwarder_type as Type, version as Version, os as OS, arch as Architecture, status as Status, last_connected as "Last Connected to Indexers", sum_kb as "Total KB", avg_tcp_kbps_sparkline as "Average KB/s Over Time", avg_tcp_kbps as "Average KB/s", avg_tcp_eps as "Average Events/s"

[data_model_summary]
search = | rest /servicesNS/-/-/data/models splunk_server=* count=0 search="disabled=0" \
| table splunk_server,title acceleration.cron_schedule eai:digest, eai:acl.app, acceleration.allow_skew, acceleration.max_time, acceleration.max_concurrent \
| rename title as datamodel  \
| rename acceleration.* AS *  \
| rename eai:acl.app AS app  \
| table splunk_server, app, datamodel, cron_schedule, allow_skew, max_time, max_concurrent

[schedule_search_summary]
search = | rest /servicesNS/-/-/saved/searches splunk_server_group=dmc_group_search_head search="is_scheduled=1 AND disabled=0"  \
| rename eai:acl.app AS app, title AS savedsearch_name  \
| table splunk_server, app, savedsearch_name, cron_schedule, schedule_window, next_scheduled_time, search

[private_accelerated_reports]
search = | rest /servicesNS/-/-/saved/searches splunk_server=vahclp02splunks.oss.central1.com search="auto_summarize=1 AND disabled=0 AND eai:acl.sharing=user" \
| table eai:acl.app, eai:acl.owner, title, auto_summarize, auto_summarize.cron_schedule,auto_summarize.dispatch.earliest_time,auto_summarize.dispatch.latest_time, auto_summarize, search

[sources_and_sourcetypes]
search = | metadata type=sourcetypes index=* \
    | rename sourcetype AS value

[adhoc_searches]
search = index=_audit action="search" search="*" NOT user="splunk-system-user" savedsearch_name="" NOT search="\'|history*" NOT search="\'typeahead*" \
|  timechart span=1m minspan=1m partial=f count by host

[license_summary]
search = |  rest servicesNS/-/-/licenser/licenses splunk_server=* \
|  table splunk_server, status, type, group_id, subgroup_id, label, creation_time, expiration_time, features, quotaGB, quota, license_hash, sourcetypes,window_period \
| eval quotaGB=quota/1024/1024/1024, quotaGB=round(quotaGB,2), expiration_time=strftime(expiration_time, "%F %T %Z") \
| rename label AS license \
| table splunk_server, type, sourcetypes, license, status, expiration_time, quotaGB

[cluster_settings_summary]
search = |  rest servicesNS/-/-//cluster/config splunk_server=* \
|  table splunk_server, cluster_label, mode, forwarderdata_rcv_port, forwarderdata_use_ssl, guid, manual_detention, multisite, site, site_replication_factor, site_search_factor, search_factor, replication_factor, replication_port, replication_use_ssl, rolling_restart, summary_replication

[search_peer_summary]
search = | rest /servicesNS/-/-/search/distributed/peers splunk_server=* \
| table splunk_server, cluster_label, guid, title, host, host_fqdn, status. version, os_name, isForwarding, is_https, peerName, peerType, replicationStatus, rtsearch_enabled, search_groups, searchable_indexe

[deployment_client_summary]
search = | rest /servicesNS/-/-/deployment/client splunk_server=* \
| table splunk_server, serverClasses, targetUri, clientNames

[deployment_clients_summary]
search = | rest /servicesNS/-/-/deployment/server/clients splunk_server=* \
|  table splunk_server, clientName, ip, dns, hostname, package, splunkVersion, utsname



